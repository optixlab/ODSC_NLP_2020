{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-aa5b4d60-bee3-4100-bc22-17dd18a1a30e","output_cleared":false,"source_hash":"ea0c3f63","execution_start":1603152215429,"execution_millis":2},"source":"# Ensure you have the right libraries","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-91fe360f-8b64-4275-9f4d-7238510d1a13","output_cleared":false,"source_hash":"970b981","execution_millis":1915,"execution_start":1603663219503},"source":"import nltk\nimport torchtext\nimport torch\nimport transformers\nimport numpy as np","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-4896cff5-9d76-4be4-865b-26913d74c59a","output_cleared":false,"source_hash":"2f335b27","execution_millis":36,"execution_start":1603255672321},"source":"import requests\n\nurl = \"http://www.gutenberg.org/files/2554/2554.txt\"\n\nresponse = requests.get(url)\nraw_html = response.content\ntext = raw_html.decode(\"utf-8-sig\")","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-6a11ed69-f795-445f-8d03-e25cd2b1bfd6","output_cleared":false,"source_hash":"df374d97","execution_millis":206,"execution_start":1603255675947},"source":"from urllib import request\n\nurl = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n\nresponse = request.urlopen(url)\nraw = response.read()\ntext = raw.decode(\"utf-8-sig\")","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NLP Scenario\n\nhttps://learning.oreilly.com/scenarios/nlp-with-python/9781492087687/","metadata":{"tags":[],"cell_id":"00004-b429557a-681e-41f9-95a6-5ee7ba31e38f"}},{"cell_type":"markdown","source":"### Bag of Words\n\nEncoding text into a Bag of Words or TF-IDF vector is a fundamental step to NLP applications. We can learn a lot from the BOW/TF-IDF vectors for documents and in turn how similar documents may be.\n\nIn future scenarios we will learn how to further use BOW/TF-IDF for text classification as well as enhanced word embeddings.","metadata":{"tags":[],"cell_id":"00005-979e2b21-505e-400f-bf9e-b4f5f714ffc6"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00006-d10ed492-8d7f-4184-87b9-a523d2de5697","output_cleared":false,"source_hash":"abe7455d","execution_start":1603604696074,"execution_millis":491},"source":"import urllib.request\nurl = 'https://www.gutenberg.org/files/2701/2701-0.txt'\n\nfile = urllib.request.urlopen(url)\ntext = [line.decode('utf-8') for line in file]\ntext = ''.join(text)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Tokenize","metadata":{"tags":[],"cell_id":"00007-25a09920-d03b-41c9-9575-a1b624805978"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-3439e997-49ba-4a24-9837-0b0aa9f8ccce","output_cleared":false,"source_hash":"dd04af38","execution_start":1603604736850,"execution_millis":4711},"source":"import nltk\nnltk.download('punkt')\nfrom nltk import word_tokenize\ntokens = word_tokenize(text)","execution_count":null,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-b11c3837-7e0d-46b7-9e68-bd894bd520b6","output_cleared":false,"source_hash":"204a6f56","execution_start":1603604749062,"execution_millis":439},"source":"import string\ntokens = [word for word in tokens if word.isalpha()]\ntable = str.maketrans('', '', string.punctuation)\ntokens = [w.translate(table) for w in tokens]\ntokens = [word.lower() for word in tokens]","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing **stop-words** and **stemming**","metadata":{"tags":[],"cell_id":"00010-629ee7ae-ef47-4016-a608-63e21608e96c"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00010-68c8702c-ffad-4aa1-9feb-f68f56e989cb","output_cleared":false,"source_hash":"203b141e","execution_millis":713,"execution_start":1603663191681},"source":"from nltk.corpus import stopwords\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\ntokens = [w for w in tokens if not w in stop_words]\n\nfrom nltk.stem.porter import PorterStemmer\nporter = PorterStemmer()\ntokens = [porter.stem(word) for word in tokens]\ntokens[200:202]","outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'nltk' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3076f1f78f2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"]}],"execution_count":null},{"cell_type":"markdown","source":"**Understanding the vocabulary** \n\n* A vocabulary of a document represents all the words in that document and the frequency they appear.\n* `FreqDist` class\n","metadata":{"tags":[],"cell_id":"00012-694897bf-d63a-452e-bb93-425b30241fbe"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-d222e8ae-6118-4b56-af08-6f988535f3d4","output_cleared":false,"source_hash":"60a04f4f","execution_millis":69,"execution_start":1603604893314},"source":"from nltk.probability import FreqDist\n\nword_counts = FreqDist(tokens)\nword_counts","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"FreqDist({'whale': 1455, 'one': 920, 'like': 590, 'upon': 567, 'ship': 553, 'ye': 521, 'man': 496, 'ahab': 495, 'sea': 461, 'seem': 460, ...})"},"metadata":{}}]},{"cell_type":"markdown","source":"**Scoring words with frequency**","metadata":{"tags":[],"cell_id":"00014-33d4e49e-244e-43b5-bbd1-c24d41a8546b"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00014-c3883cfe-56a0-44db-99a2-54dcb9a5923c","output_cleared":false,"source_hash":"d999d5d7","execution_start":1603604960512,"execution_millis":1},"source":"top = 500\nvocabulary = word_counts.most_common(top)\n\nvocabulary[:10]","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"[('whale', 1455),\n ('one', 920),\n ('like', 590),\n ('upon', 567),\n ('ship', 553),\n ('ye', 521),\n ('man', 496),\n ('ahab', 495),\n ('sea', 461),\n ('seem', 460)]"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00016-e6e52561-5772-4c8f-8afb-df06ce7da7d5","output_cleared":false,"source_hash":"a2651e8b","execution_millis":2,"execution_start":1603605027818},"source":"voc_size = len(vocabulary)\ndoc_vector = np.zeros(voc_size)\n\nword_vector = [(idx,word_counts[word[0]]) for idx, word in enumerate(vocabulary) if word[0] in word_counts.keys()] \nword_vector[10]","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"(10, 443)"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00017-abcc13be-89c6-46a5-b5e2-d44716180f67","output_cleared":false,"source_hash":"ae320f3a","execution_start":1603605148964,"execution_millis":325},"source":"## making a bag of words model\n\nfrom nltk import sent_tokenize\n\ndocs = sent_tokenize(text)[703:706]\ndocs\n\n#Then we want to import a helper module from SciKitLearn called CountVectorizer with:\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# The helper CountVectorizer can tokenize, clean, and count all the tokens in our documents with:\n\ncount_vectorizer=CountVectorizer(stop_words='english')\n\nword_count_vector=count_vectorizer.fit_transform(docs)\nword_count_vector.shape\n\n# The shape of the word_count_vector represents the number of documents (3) and total number of words (17) in those documents.\n\n# The word_count_vector represents the documents bag of words. We can view the contents of this by:\n\nword_count_vector.toarray()\n\n# Where each 1D vector represents positions for the entire vocabulary. Each value of 1 represents that word is present in the document or in this case sentence.\n\n# We can view that list of words by querying the count_tokenizer with:\n\ncount_vectorizer.get_feature_names()","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"['bedwards',\n 'began',\n 'coming',\n 'decent',\n 'getting',\n 'going',\n 'harpooneer',\n 'hole',\n 'home',\n 'late',\n 'midnight',\n 'ought',\n 'suppose',\n 'tell',\n 'tumble',\n 'twitch',\n 'vile']"},"metadata":{}}]},{"cell_type":"markdown","source":"## PyTorch CBOW\n\nhttps://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html","metadata":{"tags":[],"cell_id":"00018-11a12ccd-791e-4178-814c-9ab8a9b54fb3"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00019-331872b7-e1c9-4849-88e2-7dc86833f8b0"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"86b021d1-04ea-4218-98f9-b06141d89a25","deepnote_execution_queue":[]}}